**TIL (Today I Learned): '할루시네이션(Hallucination)' in AI**

---

## 1. '할루시네이션'이란?
- 일반적으로 **인간의 정신세계**에서 나타나는 **환각**(실제 존재하지 않는 것을 지각) 현상을 뜻하지만,  
- **인공지능(특히 대형 언어 모델, LLM) 분야**에서는, 모델이 실제로 존재하지 않거나 근거가 없는 정보를 **그럴듯하게** 만들어내는 오류를 가리켜 ‘할루시네이션’이라고 부릅니다.  
- 쉽게 말해, **AI가 거짓 정보를 사실처럼 생성**하거나, **실존하지 않는 출처**를 제시하는 문제라고 할 수 있어요.

---

## 2. 왜 이런 현상이 발생할까?

1. **확률적 예측 메커니즘**  
   - 대형 언어 모델(예: GPT, Bard 등)은 입력(프롬프트)에 따라 **가장 그럴듯한 단어 시퀀스**를 확률적으로 생성  
   - 이 과정에서, **사실 확인**이나 **정합성 검증**이 제대로 되지 않아 **허구의 문장**을 완성할 수 있음

2. **훈련 데이터 편향**  
   - 모델이 학습된 방대한 텍스트 중에 **오류 정보**나 **허위 사실**이 섞여 있으면, 그것을 **사실**처럼 학습할 가능성  
   - 결과적으로 **잘못된 지식**을 재생산하기 쉬움

3. **불완전한 내부 검증**  
   - 대형 언어 모델이 **논리 추론**보다는 **패턴 매칭** 위주로 동작  
   - 자가 검증(스스로 사실 여부를 체크) 능력이 한계가 있으므로, 그럴싸해 보이는 **거짓**을 말해버릴 수 있음

---

## 3. 예시 사례

1. **가짜 정보**  
   - “지구 상에서 가장 큰 포유류는 하마입니다” 같은 명백한 잘못된 답변을 **자신감 있게** 말하는 경우  
   - 사용자가 추가 질문을 해도 계속 **허위 근거**를 제시할 수 있음

2. **존재하지 않는 출처**  
   - 논문 인용이나 서적 정보를 요청했더니, **실제로 존재하지 않는 저자명, 출판사명**을 만들어내거나, **날조된 링크**를 생성

3. **의도된 모순**  
   - 하나의 질문에 대해 답변이 여러 번 바뀌고, 앞뒤가 안 맞는 설명이 반복되는 경우  
   - 논리적으로 말이 안 되는 결과물을 자신 있게 제공

---

## 4. 문제점과 위험성

1. **잘못된 정보 유포**  
   - 유저가 AI 답변을 그대로 믿고 활용하면, **지식 오류**나 **오보**가 퍼질 수 있음  
2. **신뢰도 하락**  
   - AI가 종종 헛소리를 한다는 인식이 생기면, **전반적인 신뢰**가 깨질 수 있음  
3. **법적·윤리적 이슈**  
   - 의료·법률 등 민감 영역에서 허위 정보를 제공하면, **심각한 피해**로 이어질 가능성

---

## 5. 대처와 개선 방향

1. **출처 검증(Verification)**  
   - 모델이 제공하는 정보의 **출처**(Reference)를 사용자에게 공개하고,  
   - 사용자가 **사실 검증**을 쉽게 할 수 있도록 돕는 UI/UX 설계

2. **사후 피드백(Feedback Loop)**  
   - 사용자가 “이 정보가 틀렸다”고 피드백을 줄 경우, **학습 과정**(파인튜닝)에서 반영  
   - 시스템적으로 **정정 요청**을 거쳐 모델이 스스로 오류를 수정하도록 시도

3. **혼합 모델 접근**  
   - LLM+지식 그래프(Knowledge Graph) 또는 외부 DB를 결합해, **팩트체크**를 강화하는 시스템 설계  
   - 예: 검색 기반(QA) 기능을 통해 실제 웹 문서나 DB에서 **정확도**를 높이는 하이브리드 모델

4. **정밀한 프롬프트 설계(Chain-of-Thought, ReAct 등)**  
   - 모델이 답변 전, **자기 자신**에게 질문하고 논리를 전개하는 기법으로 **추론의 오류**를 줄이려는 연구가 진행 중

---

## 6. 오늘 배운 점

- “할루시네이션”이라는 용어가 AI 영역에서도 쓰인다는 사실이 참 흥미롭다.  
- 결국, 대형 언어 모델이 **왜곡된 문장**이나 **허위 정보**를 그럴듯하게 만들어낸다는 게 핵심 문제인데, 이는 모델 구조상 **패턴 예측**에 치중하고 **사실 검증** 기능이 부족하기 때문으로 보인다.  
- 이를 개선하기 위해서는 **추가적인 검증 시스템**이나, **책임감 있는 활용(Responsible Use)**, 그리고 **사용자 피드백** 등 다각도의 노력이 필요한 듯하다.

---

### 한 줄 요약
“AI의 할루시네이션은 **실제와 무관하거나 거짓된 정보를 생성**하는 현상으로, 대형 언어 모델의 **패턴 예측 특성**에서 기인하며, 사실 검증·피드백 등을 통해 개선하려는 연구가 활발하다.”