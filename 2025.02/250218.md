**TIL (Today I Learned): 선형 회귀(Linear Regression)**

---

## 1. 선형 회귀란?
- **독립 변수**(x)와 **종속 변수**(y) 간의 관계를 **직선**(혹은 초평면)으로 모델링하는 기법  
- 데이터에 **가장 잘 맞는 선**(예: y = w*x + b)을 찾아내어, **연속적인 값을 예측**하는 데 사용  
- 통계학, 머신러닝 등에서 **기초 모델**로 매우 자주 등장

---

## 2. 기본 아이디어
1. **가정**  
   - 종속 변수 y는 독립 변수 x에 대해 **선형적**(linear)으로 변화한다고 가정  
   - 예: y = w*x* + b (단일 선형 회귀), y = w1*x*1 + w2*x*2 + ... + b (다중 선형 회귀)

2. **목표**  
   - w(가중치), b(절편)을 찾아서 데이터 포인트들에 대해 **오차**(실제값 - 예측값)를 최소화  
   - 주로 **최소제곱법(Ordinary Least Squares)** 기반으로, **제곱 오차**(SSE)를 최소화하는 파라미터를 구함

3. **결과 해석**  
   - 얻어진 w, b는 “x가 1 증가할 때 y가 w만큼 변화한다”와 같은 식으로 **상관관계**를 해석 가능  
   - 예측할 때는 x에 대입해서 y값을 얻음

---

## 3. 비용 함수와 최적화

1. **비용 함수(Cost Function)**
   - 일반적으로 **평균제곱오차(MSE)** 사용  
   - MSE = \(\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)  
   - y_i: 실제값, \(\hat{y}_i\): 예측값 (w*x_i + b)

2. **최적화 방법**
   - **정규방정식(Analytical solution)**: \(\mathbf{w} = (X^T X)^{-1} X^T \mathbf{y}\) 방식  
   - **경사하강법(Gradient Descent)**: 대규모 데이터나 다차원에서는 수치적으로 반복 계산해 w, b 갱신

---

## 4. 단순 vs. 다중 선형 회귀

1. **단순 선형 회귀(Simple Linear Regression)**  
   - 독립 변수(특징)이 1개인 경우  
   - y = w*x + b 형태  
   - 이차원 평면에서의 “직선”을 찾는 문제

2. **다중 선형 회귀(Multiple Linear Regression)**  
   - 독립 변수가 여러 개  
   - y = w1*x*1 + w2*x*2 + ... + wn*x*n + b  
   - 고차원 공간에서의 “초평면”을 찾는 문제

---

## 5. 선형 회귀의 전제 조건(가정)

1. **선형성(Linearity)**  
   - 독립 변수와 종속 변수 간 관계가 선형적이어야 함 (큰 편차가 있으면 부적합)  

2. **독립성(Independence)**  
   - 샘플 간 상호 독립, 잔차(오차) 간 자기상관 없음  

3. **등분산성(Homoscedasticity)**  
   - 잔차의 분산이 일정해야 함 (잔차가 x에 따라 크게 변하지 않아야 함)

4. **정규성(Normality)**  
   - 잔차가 정규 분포를 따른다는 가정 (회귀 해석 및 통계적 유의성 검증에 유리)

---

## 6. 한계 및 보완책

1. **선형성 가정의 제한**  
   - 실제 데이터가 비선형 구조일 때는 모델이 부적합  
   - 해결: **다항 회귀(Polynomial Regression)** 등으로 확장 가능

2. **과적합/다중공선성**  
   - 독립 변수 간 강한 상관관계가 있으면 추정 불안정  
   - 해결: **정규화 기법**(Ridge, Lasso 등) 사용

3. **이상치(Outliers)**  
   - 선형 회귀는 이상치에 민감 (MSE를 쓰니, 큰 오차가 잘 돋보임)  
   - 해결: 강건 회귀(Robust Regression), 이상치 제거/처리

---

## 7. 오늘 배운 점

1. 선형 회귀는 데이터 분석/머신러닝에서 **기초 중의 기초** 모델이지만, 여전히 **강력**하고 **해석**이 쉬워서 널리 쓰임을 다시 확인  
2. 단순히 y=wx+b를 찾는 단순 구조지만, 실제로는 **다중 변수, 편향, 이상치** 등 고민 요소가 많아 실제 적용 시 주의가 필요  
3. 경사하강법, 정규방정식 등 학습(훈련) 방식이 여러 가지 있지만, 데이터 규모에 따라 적절한 방법을 선택하는 것이 효율적  
4. 모델 해석할 때 가정(독립성, 정규성, 등분산성 등)이 중요하며, 위배 시 회귀 계수의 통계적 의미가 왜곡될 수 있음

---

### 한 줄 요약
“선형 회귀는 **독립 변수와 종속 변수의 선형 관계**를 모델링해 **연속적 값을 예측**하는 핵심 기법이며, 간단하지만 해석이 쉬운 만큼 데이터 특성(선형성, 가정 등)을 잘 이해하고 적용해야 한다.”