<<<<<<< HEAD
### RNN   

#### 관련 주요 용어   
- 시퀀스(sequence) : 항목(item)들의 나열   
- 양방향 RNN(bi-directional RNN) : 은닉 상태를 통한 정보의 흐름이 시간 흐름 및 그 역방향으로 모두 존재하는 RNN   
- BPTT(BackPropagation Through Time) : RNN에서 파라미터의 업데이트가 시간을 거꾸로 거슬러 올라오는 형태로 이루어지는 기법   
- 망각 게이트(forget gate) : 장기기억 벡터에서 어떤 정보를 잊어야 할지 결정하는 게이트   
- 입력 게이트(input gate) : 입력 중 어떤 정보가 중요하고 기억해야 하는지 결정하는 게이트   
- 출력 게이트(output gate) : 출력을 만들 때 어떤 정보만 남겨야 할지 결정하는 게이트   

#### What is RNN?   
&ensp;RNN은 이미지보다는 시퀀스 작업에 적합한 모델이다. 학습이 끝난 뒤에는 학습된 파라미터를 반복적으로 적용해서 임의의 길이를 가진 시퀀스를 입력으로 받을 수 있다. 시퀀스에 대한 정보를 은닉 상태로 저장하여 다음 시점으로 전달한다. 학습을 위해서는 RNN을 펼쳐놓은 상태에서 오차역전파 방법을 적용하는 것과 같은 방식으로 파라미터들을 업데이트하게 된다.   

&ensp;LSTM이나 GRU 셀을 사용하지 않은 기본적인 **RNN의 단점**이라면 <u>입력 시퀀스의 길어짐에 따라 모델의 성능이 저하</u>될 수 있으며, 이같은 단점을 경감시키기 위해서 **LSTM**과 **GRU**가 등장하게 되었다.  

&ensp;매 시점의 은닉 상태는 이전 시점의 은닉 상태와 현 시점의 입력값을 종합하여 결정하게 된다.   

&ensp;망각 게이트(forget gate), 입력 게이트(input gate), 출력 게이트(output gate)가 LSTM 셀을 구성하기 위해 사용된다.   

&ensp;LSTM에 사용되는 게이트 중 하나인 망각 게이트(forget gate)는 이전 시점에서 넘어온 장기 기억 벡터 c에 대한 필터로 작용하여, 이 중 어떤 부분을 잊어야 하는지 결정한다.   

#### Today's RNN   
1. RNN은 시퀀스를 다루기 위한 <u>신경망 모델</u>이다.   
2. 시퀀스를 이루는 항목들이 순차적으로 모델에 입력으로 들어가며, 이전 시점의 입력으로부터 얻어 낸 정보를 다음 시점으로 전달해서 추론에 사용한다.   
3. 입력으로 들어오는 시퀀스의 길이가 학습 때 사용된 최대 시퀀스 길이보다 더 길어진다 하더라도 같은 파라미터들을 반복적으로 적용하여 모델의 출력을 계산해 낼 수 있다.   
4. 특정 시점의 추론을 수행하기 위해서 해당 시점 앞, 뒤의 정보가 모두 필요한 경우, 정보를 양방향으로 흐르게 하는 RNN을 사용하는데 이와 같은 형태를 **양방향(bi-directional) RNN**이라고 한다.   
5. LSTM은 상대적으로 중요한 정보를 선택적으로 기억함으로써 시퀀스가 길어질 때 RNN에서 생기는 <u>성능 저하 문제를 완화</u>할 수 있다.   
6. 매 시점마다 각각 <u>장기/단기 기억</u>에 해당하는 2개의 벡터를 유지한다.   
7. GRU는 LSTM의 <u>단순화</u> 버전으로 하나의 상태 벡터만을 유지하며, 출력 게이트가 제거되었다.
=======
#### YOLO API을 활용한 오브젝트 탐지 실습   
욜로를 사용해 본인이 갖고 있는 이미지의 객체를 탐지하는 코드   
**YOLO 모델 로드**
```python
import cv2
from ultralytics import YOLO

# YOLO 모델 로드 (원하는 모델 경로 지정)
model = YOLO('yolov8n.pt') # YOLOv8n 모델을 예시로 사용
```   
**이미지 불러오기 및 시각화**   
```python
# 이미지 불러오기
image_path = 'cat.jpeg' # 이미지 경로 지정
img = cv2.imread(image_path)

# 객체 탐지 수행
results = model(img)

# 결과 시각화 (바운딩 박스, 클래스 이름, 신뢰도)
for result in results:
    boxes = result.boxes
    for box in boxes:
        # 클래스, 신뢰도 정보 얻기
        class_id = box.cls[0].item()
        confidence = box.conf[0].item()

        # 바운딩 박스 좌표 얻기
        x1, y1, x2, y2 = box.xyxy[0].tolist()

        # 바운딩 박스 및 정보 그리기
        cv2. rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2) # 초록색 박스'
        label = f"{model.names[class_id]}: {confidence: 2f}"
        cv2.putText(img, label, (int(x1), int(y1 - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
```   
**결과 이미지 출력**   
```python
cv2.imshow("YOLOv8 Object Detection", img)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
>>>>>>> 11674cd6a650c267d63219c423efdecad5176ff9
