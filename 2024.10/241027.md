  ### 실무에서 사용할 대비를 하기 위해 데이터 전처리와 토큰화, 패딩을 연습
**요구 사항**   
문제 1: 가짜 데이터 생성 및 전처리   
(1) 가짜 텍스트 데이터를 생성합니다.   
```python
# 가짜 데이터 생성
np.random.seed(42)
num_sample = 1000
texts = ["This is a positive review." if i % 2 == 0 else "This is a negative review." for i in range(num_samples)]
labels = [1 if i % 2 == 0 else 0 for i in range(num_samples)] 
```   
(2) 텍스트 데이터를 전처리하여 분석을 위한 준비를 합니다.   
문제 2: 토큰화 및 패딩   
(1) 텍스트 데이터를 토큰화하고 패딩하여 모델 입력 형태로 변환합니다.   
```python
import numpy as np
from sklearn.model_selection import train_test_split
import re
from transforms import BertTokenizer
import torch

# 가짜 데이터 생성
np.random.seed(42)
num_samples = 1000
texts = ["This is a positive review." if i % 2 == 0 else "This is a negative review." for i in range(num_samples)] # 긍정 리뷰와 부정 리뷰를 번갈아 생성
labels = [1 if i % 2 == 0 else 0 for i in range(num_samples)] # 라벨은 긍정 리뷰에 1, 부정 리뷰에 0을 할당

# 생성된 데이터를 훈련 데이터와 검증 데이터로 분할
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# 데이터 전처리 함수
def preprocess_text(text):
    text = text.lower()  # 모든 텍스트를 소문자로 변환
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)  # 텍스트에서 알파벳과 숫자, 공백을 제외한 모든 문자를 제거
    return text

# 전처리 적용
train_texts = [preprocess_text(text) for text in train_texts]
val_texts = [preprocess_text(text) for text in val_texts]

# 전처리된 데이터 확인
print("전처리된 훈련 데이터 예시:", train_texts[:5])
print("토큰화 및 패딩된 검증 데이터 예시:", val_texts[:5])

# transformers 라이브러리의 BertTokenizer를 사용하여BERT 토크나이저 로드
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #  BERT 토크나이저를 로드

# 텍스트 토큰화 및 패딩
def tokenize_and_pad(texts, tokenizer, max_len):
    # tokenizer를 사용하여 텍스트 데이터를 토큰화하고 패딩
    # truncation=True를 사용하여 최대 길이를 초과하는 시퀀스를 잘라냄.
    # padding=True를 사용하여 시퀀스를 동일한 길이로 패딩
    # max_length를 설정하여 패딩된 시퀀스의 최대 길이를 지정
    # return_tensors='pt'를 사용하여 결과를 PyTorch 텐서로 반환
    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors='pt') 
    return encodings

max_len = 20
train_encodings = tokenize_and_pad(train_texts, tokenizer, max_len)
val_encodings = tokenize_and_pad(val_texts, tokenizer, max_len)

# 결과 확인
print("토큰화 및 패딩된 훈련 데이터 예시:")
print(train_encodings['input_ids'][:5])
print("토큰화 및 패딩된 검증 데이터 예시:")
print(val_encodings['input_ids'][:5])
```