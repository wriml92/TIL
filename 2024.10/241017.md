#### Kaggle에서 Titanic dataset을 다운로드 후 로드

```python
import pandas as pd  # 판다스 불러오기

# 데이터 로드
train_data = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')
```

#### 전처리를 진행 후 레이블 분리 작업

```python
from sklearn.model_selection import train_test_split  # 데이터셋 분리 함수

# 필요한 열 선택 및 전처리
df = train_data[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]
df.loc[:, 'Age'] = df['Age'].fillna(df['Age'].mean())
# 레이블 분리
X = df.drop('Survived', axis=1)
y = df['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

#### 분류모델들

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# 모델 정의
models = {
    'Logistic Regression': LogisticRegression(max_iter=200),  # 로지스틱 회귀
    'Decision Tree': DecisionTreeClassifier(),  # 의사결정나무
    'Random Forest': RandomForestClassifier(),  # 랜덤 포레스트
    'Support Vector Machine': SVC(),  # SVM
    'K-Nearest Neighbors': KNeighborsClassifier()  # KNN
}
```

#### 성능 비교

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 모델 학습 및 평가
results = {}
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    results[model_name] = {
        'Accuracy': accuracy_score(y_test, y_pred),  # 정답률
        'Precision': precision_score(y_test, y_pred),  # 적합율
        'Recall': recall_score(y_test, y_pred),  # 재현율
        'F1 Score': f1_score(y_test, y_pred)  # f1 측정 값
    }

results_df = pd.DataFrame(results).T
print(results_df)
```

```python
# 결과 화면
                        Accuracy  Precision    Recall  F1 Score
Logistic Regression     0.731844   0.770833  0.500000  0.606557
Decision Tree           0.642458   0.583333  0.472973  0.522388
Random Forest           0.703911   0.661538  0.581081  0.618705
Support Vector Machine  0.653631   0.750000  0.243243  0.367347
K-Nearest Neighbors     0.675978   0.642857  0.486486  0.553846
```

---

#### 필요한 열 선택 후 레이블 분리

```python
# 필요한 열 선택 및 결측치 제거
data = train_data[['Survived', 
'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]
data = data.dropna()

# 레이블 분리
labels = data['Survived']
data = data.drop('Survived', axis=1)
```

#### 실제 레이블과 군집 비교 및 평가

```python
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, homogeneity_score, completeness_score

# KMeans 모델 초기화 (군집 수 2로 설정)
kmeans = KMeans(n_clusters=2, random_state=42)

# 모델 적합
kmeans.fit(data)

# 군집 예측
clusters = kmeans.predict(data)

#평가 지표
ari = adjusted_rand_score(labels, clusters)
homogeneity = homogeneity_score(labels, clusters)
completeness = completeness_score(labels, clusters)

print(f"Adjusted Rand Index: {ari}")
print(f"Homogeneity: {homogeneity}")
print(f"Completeness: {completeness}")
```

```python
# 결과 화면
Adjusted Rand Index: 0.03929870370135212
Homogeneity: 0.026026523392495605
Completeness: 0.07134214091774442
```