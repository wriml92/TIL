## 초거대 언어 모델   

- 언어 모델(language model) : 단어의 등장 확률을 예측하는 모델   
- n-gram 모델 : 주어진 n-1개의 단어를 가지고 n번째에 올 단어를 예측하는 모델   
- 사전 학습(pre-training) : 모델이 세부 분야에 대한 학습에 앞서 보다 일반적인 지식을 학습할 수 있도록 하는 것   
- 미세 조정(fine-tuning) : 문제를 해결하고 싶은 세부 분야에 맞게 사전 학습된 모델을 조정하는 것   
- 프롬프트(prompt) : 모델이 답을 끌어 낼 수 있게 유도해 주기 위해 제공하는 텍스트 일부   
- 파이프라인(pipeline) : 여러 단계로 되어 있는 모델들을 사용하기 쉽게 추상화시켜 놓은 것   

### 언어 모델의 발전   
#### 언어 모델(lanuage model)   
- 단어의 등장 확률을 예측하는 모델   
- 주어진 n-1개의 단어를 가지고 n번째에 올 단어를 예측하는 n-gram 모델이 언어 모델 예시 중 하나   
- 언어 모델에 반복적으로 다음 단어를 예측하게 함으로써 생성형 AI 구현이 가능   

#### GPT와 BERT의 차이   
- GPT는 트랜스포머 중 디코더 부분만을 사용해서 모델을 만드며 n-gram 형태의 모델임   
- BERT는 트랜스포머 중 인코더 부분만을 사용하여 모델을 생성하며, 문장 곳곳을 랜덤하게 마스크로 가려 놓은 문장 A, B를 주고 1️⃣ 가려 놓은 부분의 단어와 2️⃣ B가 A 다음에 이어지는 문장이 맞는지를 맞추도록 모델을 학습   
- GPT와 BERT 둘 모두 사전 학습(pre-training) 후 미세 조정(fine-tuning) 단계를 거쳐 모델을 생성   

#### GPT-2   
- 미세 조정 없이 언어 모델 하나만으로 여러 가지 태스크를 수행할 수 있도록 시도   
- GPT와 구조가 유사하나, 모델을 구성하는 디코더 층 수가 증가   
- 파라미터 수를 늘려 감에 따라 모델의 성능도 증가   

#### GPT-3   
- GPT-2와 같이 언어 모델 하나만으로 여러 가지 태스크를 수행할 수 있도록 하였음   
- GPT-2와 구조가 유사하나, 희소 어텐션(sparse attention)을 섞어서 사용   
- 프롬프트의 방식을 Zero-Shot, One-Shot, Few-Shot으로 나누어 실험   
- 파라미터 수가 일정 수준 이상으로 커짐에 따라 기존 SOTA 모델의 성능을 넘어서는 결과를 보임   

### ChatGPT   
#### GPT-3를 이용해 챗봇을 구현할 때 생기는 다음과 같은 문제들을 해결하기 위해 등장하였다.   
- 사실이 아닌 내용을 지어내는 경우가 있음   
- 편향이 있거나 무례한 답변을 만들어 냄   
- 모델에 내린 지시를 제대로 따르지 않을 수 있음   

#### ChatGPT는 다음과 같은 단계를 거쳐 훈련되었다.   
- 데이터셋으로부터 프롬프트들을 추출해 낸 뒤, 사람의 프롬프트에 대해 바람직한 답안을 수기로 작성하여 데이터셋을 생성하고, 이 데이터셋을 가지고 GPT-3를 미세 조정 → SFT(Supervised Fine-Tuning)   
- 사람을 대신해서 교정을 수행할 수 있는 별도의 모델을 생성 → RM(Reward Model)   
- RM으로 하여금 SFT를 계속해서 교정하도록 함