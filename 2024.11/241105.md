# 합성곱 신경망(Convolution Neuron Network)   

### 주요 용어 정리   
- 합성곱(convolution) : 입력 이미지와 필터를 이용하여 출력값을 계산하는 연산. 필터는 일정한 크기를 가지며, 입력 이미지의 모든 위치에서 적용됨. 입력 이미지와 필터를 곱한 후, 그 결과를 모두 더하여 출력값을 계산하고, 이를 통해 입력 이미지의 특징을 추출하는 데 사용됨   
- 스트라이드(strides) : 합성곱을 수행할 때, 필터가 이동하는 간격. 스트라이드가 크면 출력 이미지의 크기가 작아지게 됨   
- 필터(filter) : 커널로도 불리며, 합성곱에서 사용되는 가중치 행렬. 입력 이미지의 특정 패턴을 감지하는 데 사용되며 일반적으로 필터의 크기는 3x3, 5x5, 7x7 등이 사용됨   
- 풀링(pooling) : 입력 이미지의 크기를 줄이는 연산. 일반적으로 최대 풀링(max pooling) 혹은 평균 풀링(average pooling)을 사용. 최대 풀링은 특정 영역에서 최댓값을 취하는 연산이고, 평균 풀링은 대상 영역의 평균을 계산하게 됨. 이미지의 크기를 축소시켜 계산량을 줄이는 효과를 얻을 수 있음   
- 패딩(padding) : 입력 이미지 주변에 특정 값을 추가하는 기법으로 출력 이미지의 크기를 유지하거나 증가시키는 효과가 있음. 또한 입력 이미지의 경계 부분에서도 합성곱을 통해 이미지의 특징을 정밀하게 추출할 수 있도록 도와줌   
- 소프트맥스(softmax) : 다중 클래스 분류(multi-class classification) 문제에서 출력값을 확률로 변환하는 활성함수. 소프트맥스 함수는 출력층에서 사용되며, 출력값이 총합이 1이 되는 특성을 갖고 있음.   
- 완전연결층(fully connected layer) : 합성곱 신경망의 마지막 부분에 해당되며, 분류 작업에서 중요한 역할을 함. 이 층에서는 전체 이미지에서 추출된 특징 정보가 하나의 벡터로 합쳐지기 때문에, 클래스 분류를 위한 다양한 특징 정보들이 복잡한 조합을 이룰 수 있게 됨. 이는 전통적인 다층 퍼셉트론의 구조를 기반으로 하기 때문에, 다양한 신경망 모델의 분류 문제에서도 널리 사용됨.   

## 합성곱의 이해   
### 합성곱의 개념   
- 합성곱은 영상처리 기술에서 가장 기본적인 연산으로, 필터(커널)를 활용하여 연산이 수행.     
- 전통적인 영상처리 분야에서 합성곱 연산은 필터의 가중치가 변하지 않는 상수로 고정되어 있지만 합성곱 신경망의 필터의 가중치는 모르는 상태에서 다양한 입력 데이터를 이용한 학습 과정을 통해 자동으로 가중치가 결정됨.   
## 합성곱 신경망의 구조   
### 합성곱 신경망의 핵심 구조   
- 일반적으로 합성곱 신경망은 입력, 합성곱 층, 풀링층, 완전연결층으로 구성   
- 입력은 데이터가 합성곱 신경망에서 처리될 수 있도록 변환되는 단계   
- 합성곱 층은 합성곱 신경망에서 가장 많은 연산이 처리되는 계층으로, 이전 층에서 전달받은 입력 데이터와 필터를 통해 연산하게 됨.   
- 풀링층은 합성곱 층에서 연산된 출력 데이터를 입력으로 받아서 크기를 줄이는 역할을 하며, 크기를 줄이기 때문에 다운 샘플링 혹은 서브 샘플링이라고도 불림.   
- 완전연결층은 한 층과 그 다음 층이 모두 연결된 상태로, 1차원 배열의 형태로 변환하여 이미지를 분류하는 데 사용되는 계층.   
### 합성곱 층의 필터(커널)의 크기   
> 합성곱 층   
```python
tf.keras.layers.Conv2D(filters, kernel_size=(?, ?), strides=(1, 1), padding='valid', activation=None)
```   
- 필터(커널)의 크기는 하이퍼파라미터로, 사용자가 다양한 값을 시도하여 결정.   
- 일반적으로 (3, 3), (5, 5), (7, 7)의 크기 사용   
### 합성곱 신경망에서 드롭아웃을 어떻게 적용할 수 있을까?   
> 합성곱 신경망   
```python
from tensorflow import keras

model = keras.Sequential([
    # 1. 첫 번째 Conv2D 레이어: 3x3 크기의 커널을 사용하여 입력 이미지(28x28x1)에 16개의 필터를 적용하여 특징 추출
    keras.layers.Conv2D(input_shape=(28, 28, 1), kernel_size=(3, 3), filters=16),
    # 2. MaxPool2D 레이어: 2x2 크기의 풀링 윈도우를 사용해 공간적 크기를 절반으로 줄임
    keras.layers.MaxPool2D(strides=(2, 2)),
    # 3. Flatten 레이어: 2D 이미지를 1D 벡터로 변환해 Dense 레이어에 입력 가능하도록 만듦
    keras.layers.Flatten(),
    # 4. Dense 레이어: 128개의 뉴런을 가진 완전 연결층 (fully connected layer), ReLU 활성화 함수 사용
    keras.layers.Dense(128, activation=tf.nn.relu),
    # 5. Dropout 레이어: 50%의 비율로 뉴런을 무작위로 비활성화하여 과적합을 방지
    keras.layers.Dropout(0.5),
    # 6. 출력층(Dense 레이어): 10개의 뉴런을 가진 소프트맥스 출력층, 10개의 클래스로 분류하는 문제에서 사용
    keras.layers.Dense(10, activation=tf.nn.softmax)
])
```   
- 드롭아웃은 과적합을 방지하기 위한 수단으로, 층을 구성하는 뉴런(노드)의 일정 비율을 비활성화시키는 기술.   
- 다양한 연구에 의하면, 합성곱 신경망에서 드롭아웃은 Flatten()을 통해 1차원 벡터 변환이 끝나고 활성함수 이후에 적용하는 것이 효과적