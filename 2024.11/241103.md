## 과적합과 규제   
### 과적합 문제의 개선   
&emsp;과적합은 훈련에 사용한 데이터에 지나치게 적합하게 학습되는 현상을 말한다. 과적합의 원인으로는 모델의 파라미터가 매우 많아 필요 이상으로 복잡도가 높아지는 것이 있다. 그래서 모델 복잡도를 낮추어 더 '단순'한 모델로 만들기 위한 처리 절차인 규제(regularization) 방식을 행한다.   
### 훈련의 조기 종료
&emsp;훈련 과정을 관찰하여 과적합이 일어나거나 더 이상의 개선이 이루어지지 않으면, 불필요한 훈련이 반복되지 않도록 훈련을 조기에 종료하는 것을 의미한다. 학습에 사용할 전체 데이터를 훈련용 집합과 검증용 집합으로 분할한다.   
&emsp;만일 훈련용 집합에 대한 손실은 감소하고 있지만, 검증용 집합에 대해서는 개선이 되고 있지 않거나 오히려 증가한다면 과적합을 의심할 수 있다. 이러한 조건이 적젏히 정한 기준을 넘어 지속될 경우 조기 종료한다. 텐서플로에서는 tf.keras.callbacks.EarlyStopping을 이용하여 조기 종료 콜백을 만들어 학습을 수행하는 fit 메소드에 전달한다.   
### 가중치의 규제   
&emsp;가중치 감쇠(weight decay)는 신경망의 가중치가 가장 작은 값을 갖도록 억제함으로써 네트워크의 복잡도에 제한을 가하는 것을 의미한다. 목적함수에 손실함수와 더불어 가중치 w의 크기에 따른 불이익 항을 추가하는 방법으로 l~2~ 규제와 l~1~ 규제가 있다. l~2~ 규제는 가중치의 l~2~ 놈(norm)의 제곱을 불이익 항으로 사용하며 l~1~ 규제는 가중치의 l~1~ 놈을 불이익 항으로 사용한다.   
### 드롭아웃(dropout)   
&emsp;모델을 학습하는 동안 적절한 확률에 따라 뉴런을 무작위로 선택하여 일시적으로 제거하는 학습 방법이다. 은닉층의 뉴런이 드롭아웃 대상이며, 입력층도 대상이 될 수 있다. 출력층 뉴런은 드롭아웃을 하지 않는다.   
&emsp;드롭아웃 비율 p는 10~50% 범위에서 지정하는 것이 일반적이며 드롭아웃 대상은 매 훈련 단계마다 무작위로 선정한다. 훈련 과정에서 뉴런이 받는 입력은 p에 따라 감소된 입력을 받는다. 훈련 단계에서는 드롭아웃이 적용된 층으로부터 입력을 받을 때 1/(1-p)를 곱하여 전체적인 입력 신호의 크기가 유지되게 한다.