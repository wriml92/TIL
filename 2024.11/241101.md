&emsp;오늘도 방통대 딥러닝 공부를 한 것 중 일부를 정리하였다.   

---

- 경사 하강법: 목적함수의 파라미터를 적절한 값으로 초기화한 후 목적함수의 경사를 따라 파라미터를 업데이트하여 최솟값에 해당되는 파라미터로 수렴하게 하는 최적화 기법   
- 미니배치 경사 하강법: 파라미터가 훈련에 사용할 전체 데이터를 작은 크기의 부분 집합으로 분할한 미니배치 단위로 업데이트되게 진행하는 경사 하강법   
- 동적 학습률: 고정된 학습률을 사용하는 것이 아니라 처음에는 비교적 큰 학습률을 사용하고, 학습의 진척에 따라 점차 학습률을 줄이는 방식   
- 경사 소멸: 심층망을 학습하는 과정에서 입력층으로 갈수록 경사의 크기가 0에 근접하여 연결 가중치의 업데이트가 진행되지 않는 상황   
- 경사 폭발: 심층망을 학습하는 과정에서 경사가 점점 더 큰 값을 가져 연결 가중치가 발산하는 문제   
- 과적합: 학습에 의한 파라미터의 조정이 학습에 제공된 데이터에 의존적으로 이루어져 일반화가 잘 되지 않는 현상   
- 조기 종료: 훈련 과정을 관찰하여 과적합이 일어나거나 더 이상의 개선이 이루어지지 않으면 불필요한 훈련이 반복되지 않도록 훈련을 조기에 종료하는 것   
- 가중치 감쇠: 신경망의 가중치가 작은 값을 갖도록 억제함으로써 네트워크의 복잡도에 제한을 가하는 것   
- 규제: 모델의 복잡도가 필요 이상으로 높아지지 않도록 하여 더 단순한 모델로 만들기 위한 처리 절차   
- 드롭아웃: 모델을 학습하는 동안 적절한 확률에 따라 뉴런을 무작위로 선택하여 일시적으로 제거함으로써 과적합 문제를 개선하기 위한 규제 기술   
- 배치 정규화: 딥러닝에서 더 빠르고 안정적으로 훈련을 진행하기 위해 데이터 집합의 평균과 분산을 조정하는 기술   

---

### 딥러닝의 최적화 문제
&emsp;최적화는 문제에 맞게 정의된 목적함수의 값이 최적의 값이 되게 하는 파라미터의 값을 찾는 문제를 뜻한다. 딥러닝에서는 손실함수를 목적함수로 사용한다. 만약 목적함수가 볼록함수라면 파라미터의 초기값에서 시작하여 목적함수의 경사를 따라 파라미터가 변화하도록 업데이트하여 극소치에 도달하게 하는 경사 하강법으로 최적화할 수 있다. 반대로 목적함수가 볼록함수가 아니라면 지역 최소치 문제나 고원 문제 등으로 인해 경사 하강법이 최적화의 성공을 보장할 수는 없다.   

### 경사 하강법의 구현   
&emsp;배치 경사 하강법은 모든 훈련용 표본으로 한 단계의 파라미터 업데이트를 위한 경사를 계산하는 방식이다. 하나의 에폭에 한 번의 파라미터 업데이트가 이루어지며, 한 번의 업데이트에 소비되는 시간이 길어서 훈련이 매우 느리게 진행된다.   
&emsp;확률적 경사 하강법(SGD)는 무작위 순서로 선택된 각각의 표본 단위로 경사를 계산하여 파라미터를 업데이트한다. 1회의 에폭에 표본 개수만큼의 파라미터 업데이트가 일어나므로 배치 방식에 비해 빠르게 최적해 근처로 훈련이 진행될 수 있으며 배치 경사 하강법에 비해 파라미터의 변화가 불규칙하게 진행된다. 이 때는 지역 최소치에서 빠져나오는 데 도움이 될 수 있으며, 극소점 근처에 도달한 상태에서도 파라미터가 계속하여 변화하여 최적의 파라미터로 수렴하지 않는 문제를 일으킬 수도 있다.   
&emsp;미니배치 확률적 경사 하강법은 전체 학습표본 집합을 미니배치라고 하는 작은 크기의 부분집합으로 분할하여 모델을 훈련한다. 파라미터의 업데이트는 미니배치 단위로 하며, SGD에 비해 불규칙성이 완화되어 최적값에 가깝게 파라미터 값이 결정될 수가 있다. 또한 계산 성능상의 이점이 큰데, 행렬 연산의 최적화를 통해 심층망의 훈련 성능을 크게 높일 수 있으며, 큰 미니배치를 사용할수록 계산 성능은 좋지만 지역 최소치 문제가 더 커질 수 있다.   
### 동적 학습률
&emsp;처음에는 비교적 큰 학습률을 사용하고, 학습의 진척에 따라 점차 학습률을 줄이는 방식이다. 계단형 감쇠 스케줄러로는 `tf.keras.optimizers.schedules.PiecewiseConstantDecay` 클래스의 인스턴스가 있으며, 지수함수 감쇠 스케줄러로는 `tf.keras.optimizers.schedules.ExponentialDecay` 클래스의 인스턴스가 있다. 그리고 다항식 감쇠 스케줄러로는 `tf.keras.optimizers.schedules.PolynomicalDecay` 클래스의 인스턴스가 있으며 임의의 스케줄러 클래스를 선언할 때는 `tf.keras.optimizers.schedules.LearningRateSchedule` 클래스의 파생 클래스를 선언하여 스케줄러를 스스로 정의할 수 있다.   
### 불안정한 경사(unstable gradient) 문제
&emsp;경사 소멸(vanishing gredient) 문제는 심층망을 학습하는 과정에서 입력층으로 갈수록 경사의 크기가 0에 근접하여 연결 가중치의 업데이트가 진행되지 않는 상황을 나타낸다. 입력의 크기가 큰 곳에서 변화가 포화되어 미분이 0에 가까운 값을 갖는 시그모이드 함수를 활성함수로 사용하는 것이 원인이 되며, 이에 개선하는 방법으로는 활성함수의 개선, 연결 가중치의 적절한 초기값 설정, 스킵 연결의 활용, 배치 정규화 적용 등의 방법이 있다.   
&emsp;경사 폭발(exploding gradient) 문제는 경사가 점점 더 큰 값을 가져 연결 가중치가 발산하는 경우가 발생하는 문제이다. 원인으로는 가중치의 초기화가 적절하지 않거나 너무 높은 학습률을 사용하는 것 등이 있으며, 이에 개선 방법으로는 배치 정규화, 경사 절단(gradient clipping)과 같은 기술이나 Adam이나 RMSProp 등과 같은 개선된 최적화 알고리즘 등을 활용하는 방법들이 있다.
### 과적합(overfitting) 문제
&emsp;심층 신경망의 풍부한 표현력으로 인해 특정 학습 데이터 집합에 과하게 학습되는 문제이다. 이에 개선할 방법으로는 드롭아웃(dropout)이나 규제(regularization) 등을 활용하는 방법들이 있다.
### 심층 신뢰망(DBN)을 이용한 연결 가중치 초기화
&emsp;DBN으로 사전 학습하여 연결 가중치를 초기화하면 랜덤 값으로 초기화하는 방법에 비해 개선된 결과를 낼 수 있다. 여기서 DBN은 제한 볼츠만 머신(RBM)과 같은 생성 네트워크(generative network)를 여러 층 쌓는 형태의 구조를 말한다. 비지도 학습을 하는 RBM을 이용하여 입력에 대한 특징을 표현하는 역할을 할 수 있도록 가중치의 값을 형성한다.   

---

&emsp;오늘은 방통대 대학원 입학 지원서를 쓰기 위해 거의 시간을 많이 썼다. 솔직하게 붙을 자신이 많이 없지만 미래의 AI 연구원이 되기 위해 그래도 도전해 보고 싶었다.