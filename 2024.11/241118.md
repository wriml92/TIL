딥러닝 관련 논문 읽기 Part 3   

---

&emsp;오늘의 논문은 옥스퍼드 대학의 Karen Simonyan과 Andrew Zisserman 이 두 사람이 2015년에 발표한 **"Very Deep Convolutional Networks for Large-Scale Image Recognition"**(VGGNet) 논문이다. 이 논문을 중심으로 내용을 알아보겠다. 이 논문은 딥러닝 연구의 중요한 이정표로 여겨지며, 특히 **컴퓨터 비전** 분야에서 큰 영향을 끼쳤다.

---

## **1. 논문의 배경 및 동기**
- 당시 (2014~2015년경) 딥러닝 기반 이미지 분류의 대표 모델은 **AlexNet**(2012)과 **ZFNet**(2013)이었다.
- AlexNet 이후, 더 깊고 복잡한 네트워크가 성능을 향상시킬 수 있음이 밝혀졌다.
- 그러나 네트워크를 깊게 만들면 **학습 불안정성**과 **과적합** 문제가 발생할 가능성이 높아진다는 단점이 있다.
- 그래서 VGGNet의 연구진은 **단순하지만 체계적인 방식으로 네트워크 깊이를 늘려 성능을 개선**할 수 있음을 보여주고자 했다.

---

## **2. VGGNet의 주요 기여**
- 네트워크의 깊이를 **16개에서 19개 층**으로 확장하여 **깊이가 깊은 네트워크**가 성능에 미치는 영향을 분석하였다.
- 심플하면서도 강력한 설계 철학:
  - 모든 **컨볼루션 필터 크기**를 \(3 \times 3\)으로 고정하였으며,
  - 모든 **풀링 필터 크기**를 \(2 \times 2\)로 고정하였다.
  - 네트워크 구조를 단순화하여 실험적으로 성능을 비교하였다.
- 이미지넷 챌린지 대회인 **ILSVRC-2014**에서 높은 성능을 기록하며 모델의 실용성을 입증했다.

---

## **3. VGGNet의 모델 구조**
### **구조적 특징**
1. **모듈화된 블록 구조**:
   - 여러 개의 \(3 \times 3\) 컨볼루션 층이 쌓여 있으며, 그 뒤에 **맥스풀링(Max Pooling) 층**이 위치해 있다.
   - 이는 네트워크가 작은 필터를 사용하면서도 **더 깊은 표현 학습**이 가능하게 만든다.
2. **점진적 채널 증가**:
   - 네트워크가 깊어질수록 필터 수(채널 수)가 증가(64 → 128 → 256 → 512)하였다.

### **VGG-16 예시**:
- 총 16개의 가중치 층으로 이루어져 있는데, 13개의 컨볼루션 층과 3개의 Fully Connected (FC) 층으로 이루어져 있다.

### **왜 \(3 \times 3\) 필터인가?**
- \(3 \times 3\) 필터를 여러 층 쌓으면 더 큰 리셉티브 필드를 효과적으로 커버할 수 있다. 예를 들어, \(3 \times 3\) 필터 3개는 \(7 \times 7\) 필터와 유사한 효과를 낼 수 있지만, **학습 파라미터 수가 적고 계산 효율성**이 높다.

---

## **4. VGGNet의 성능**
- ILSVRC-2014 데이터셋에서 **상위 5% 오류율**(Top-5 Error Rate) 기준으로 높은 성능을 기록했다.
- **단점**:
  - 연산량이 많고 메모리 요구량이 높아, 실시간 애플리케이션에 적합하지 않을 수 있다.

---

## **5. VGGNet의 영향**
- VGGNet은 이후 모델 설계의 **기초 블록**으로 활용되었다.
  - **ResNet**과 같은 최신 네트워크의 발전에 영감을 제공했다.
  - 전이 학습(Transfer Learning)에서 강력한 백본(Backbone)으로 활용했다.
- **단순함과 일반화 가능성**이라는 설계 철학을 확립했다.

---

## **6. 논문의 한계 및 발전 방향**
- VGGNet의 큰 모델 사이즈와 연산 비용은 **ResNet**, **MobileNet** 같은 모델에서 개선되었다.
- 하지만, 학습 데이터가 부족하거나 전이 학습이 필요한 환경에서는 여전히 유용한 구조이다.