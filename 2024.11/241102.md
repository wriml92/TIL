&emsp;오늘도 이어서 방통대 딥러닝에 대해 정리한다.  

---

### 글로롯(Xavier Glorot) 등의 초기화 방법   
&emsp;뉴런의 팬-인(fan-in)과 팬-아웃(fan-out)에 따라 결정되는 값의 범위에 속하는 랜덤 값으로 초기화하면 불안정한 경사 문제를 개선하는 좋은 결과를 얻을 수 있다.   
- tf.keras.initializers.GlorotUniform   
- tf.keras.initializers.GlorotNormal   
### 헤(Kaiming He) 등의 초기화 방법   
- 팬-인을 바탕으로 하여 정해지는 랜덤 값에 따라 가중치를 초기화한다.   
- tf.keras.initializers.HeUniform   
- tf.keras.initializers.HeNormal   
### 모멘텀의 활용   
&emsp;역전파 과정에서 가중치를 업데이트할 때 이전 업데이트 양(속도)을 일정 비율(모멘텀)로 반영하는 방법이다.   
```python
# 예시 코드
optimizer = tf.keras.optimizers.SGD(0.1, momentum=0.9)
```
&emsp;네스테로프 가속 경사(Nesterov Acclerated Gradient, NAG)는 경사를 계산할 때 현재의 가중치 w가 아니라 모멘텀만큼 이동한 위치에서 경사를 구함으로써 좀 더 정확한 경사의 방향을 적용할 수 있게 한다.   
### Adagrad와 RMSProp 최적화   
&emsp;경사 하강법은 목적함수의 가장 가파른 경사를 따라 파라미터를 업데이트한다. 극소점을 향하여 파라미터가 변화하지 않고 가파른 경사를 따라 진행하다가 방향을 바꿔 서서히 극소점을 향해 움직인다.   
&emsp;Adagrad(Adaptive gradient)는 학습률을 적응적으로 적용하기 위한 최적화 방법이다. 변화가 큰 파라미터의 학습률을 작게, 변화가 작은 파라미터의 학습률은 크게 함으로써 극소점을 향해 파라미터의 변화가 진행되게 하려고 한다.   

---

&emsp;요즘 번아웃이 와서 그런지 코드에 관심이 점점 적어진다. 그리고 TIL을 더 잘 쓰고 싶은데 의욕이 점점 떨어지고 있다. 큰일이다. 