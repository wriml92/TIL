## 심층 합성곱 신경망   
   
- 방사형 기저 함수(Radial Basis Function, RBF) : 가우시안 연결로도 불리며, 입력값과 가중치 사이의 거리에 기반하여 값을 계산하는 함수. 일반적으로 비선형 방정식을 근사하기 위해 사용되며, 높은 차원의 입력 데이터를 저차원 공간으로 사상하는 데도 사용   
- 오버래핑 최대 풀링 : 일반적인 최대 풀링과 유사하지만, 이전 풀링 영역과 겹치는 부분을 포함하여 풀링을 수행하는 방법   
- LRN(Local Response Normalization) : 특정 위치의 픽셀값이 높을 때, 인접한 픽셀에 영향을 주는 것을 방지하고자 다른 특징 맵의 동일 위치에 있는 값을 통해 정규화하는 방법   
- 인셉션 모듈 : 다양한 크기의 합성곱 층의 결과와 최대 풀링층의 결과를 연속된 하나의 값으로 연결한 구조   
- 잔차 학습(residual learning) : 잔차 학습은 입력값과 출력값의 차이, 즉 잔차를 학습하는 방법임. 숏컷 연결을 통해 각각의 블록들이 작은 정보만을 추가적으로 학습할 수 있는 구조   

### LeNet-5      
- 얀 르쿤 연구팀이 1998년에 개발한 합성곱 신경망 모델   
- 입력, 합성곱 층 3개(C1, C3, C5), 평균 풀링층 2개(S2, S4), 완전연결층(F6), 출력으로 구성된다. 합성곱 층과 완전연결층의 활성함수로 tanh를 적용하였다.   

### AlexNet      
- 알렉스 크리체프스키, 일리아 서스케버, 제프리 힌턴이 개발하였으며, 2012년에 개최된 ILSVRC 대회에서 우승을 차지한 모델   
- 5개의 합성곱 층과 3개의 완전연결층으로 구성   
- 마지막 층에서는 1,000개의 레이블을 분류하기 위한 활성함수로 소프트맥스를 사용   
- 주요 특징이라면 2개의 GPU로 연산을 처리할 수 있는 병렬적인 구조로 설계되었다는 점   

### VGGNet  
- 2014년에 옥스퍼드 대학의 Visual Geometry Group(VGG) 연구소에서 개발된 모델로, ILSVRC-2014 대회에서 준우승을 차지   
- 높은 성능을 위해 작은 크기의 3x3 필터와 최대 풀링을 사용하여, 모델의 층을 깊게 구성   
- 각 층은 비교적 단순한 형태로 구성되어 있기 때문에 구현과 이해가 쉬움   

### GoogLeNet
- 2014년에 구글 연구팀에서 개발된 모델로, ILSVRC-2014 대회에서 우승을 차지   
- 인셉션 모듈이라는 새로운 구조를 설계하여 성능을 향상시켰음   
- 인셉션 모듈은 합성곱 층을 여러 개의 크기로 분리하여 병렬적으로 연결한 구조로, 이를 통해 이미지의 다양한 특징 추출 및 효율적인 연산을 수행   

### ResNet   
- 2015년에 마이크로소프트 북경 연구소에서 개발된 모델로, ILSVRC-2015 대회에서 우승을 차지   
- 모델이 깊어짐에 따라 발생하는 경사 소멸 문제를 해결하기 위하여 잔차 학습을 도입   
- 잔차 학습은 기존의 학습 방법과 다르게 입력에서 출력으로 연결되는 숏컷 연결이 추가된 형태   
- 깊은 층에서도 최적화가 가능해졌으며, 정확도를 개선할 수 있게 됨   