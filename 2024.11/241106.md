#### 두 문장 간의 BERT 임베딩을 사용하여 코사인 유사도를 계산하는 코드
```python
# 필요한 라이브러리 및 모델 임포트
from transformers import BertModel, BertTokenizer
import torch
from scipy.spatial.distance import cosine

# 사용할 BERT 모델의 이름 지정
model_name = "bert-base-uncased"

# 토크나이저와 모델 로드
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# 비교할 두 문장 정의
sentences = [
    "The quick brown fox jumps over the lazy dog",
    "A fast brown fox leaps over a sleepy dog"
]

# 첫 번째 문장을 토크나이저로 인코딩하여 텐서로 변환
input1 = tokenizer(sentences[0], return_tensors='pt')

# 두 번째 문장을 토크나이저로 인코딩하여 텐서로 변환
input2 = tokenizer(sentences[1], return_tensors='pt')

# 모델을 사용하여 각 문장의 임베딩 생성 (gradient 계산을 비활성화)
with torch.no_grad():
    output1 = model(**input1)
    output2 = model(**input2)

# BERT의 출력은 각 토큰의 임베딩이 포함된 텐서입니다.
# 이를 평균하여 각 문장의 전체 임베딩을 구함
embedding1 = output1.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()
embedding2 = output2.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

# 코사인 유사도 계산 (1에서 코사인 거리 값을 빼면 유사도가 됨)
similarity = 1 - cosine(embedding1, embedding2)

# 결과 출력
print(f"Cosine similarity between the two sentences: {similarity:.4f}")
```
1. **토크나이저 및 모델 불러오기**   
&emsp;BertTokenizer와 BertModel을 로드합니다. 이때 bert-base-uncased는 소문자로 변환된 일반적인 영어 텍스트에서 학습된 BERT 모델을 사용합니다.   
   
2. **문장 정의 및 토크나이저 적용**   
&emsp;비교할 두 문장을 설정한 후, tokenizer로 각 문장을 토큰화하고 이를 PyTorch 텐서 형태로 반환합니다. 텐서로 변환함으로써 모델에 입력할 수 있게 됩니다.   

3. **임베딩 계산**   
&emsp;torch.no_grad() 블록 내에서 모델을 사용하여 각 문장의 임베딩을 계산합니다. 여기서 output1.last_hidden_state와 output2.last_hidden_state는 각 문장의 모든 토큰의 임베딩을 포함한 텐서를 반환합니다.   

4. **문장 임베딩 계산**   
&emsp;각 토큰의 임베딩 값을 평균 내어 문장 단위의 임베딩을 만듭니다. 이를 통해 문장의 의미를 요약하는 하나의 벡터가 생성됩니다.   

5. **코사인 유사도 계산**   
&emsp;scipy.spatial.distance의 cosine 함수를 사용해 두 문장의 임베딩 간 코사인 거리를 구하고, 1에서 빼서 유사도로 변환합니다.   

6. **유사도 출력**   
&emsp;최종적으로 계산된 유사도를 소수점 넷째 자리까지 출력하여 두 문장의 의미적 유사도를 확인합니다.   

---

#### Facebook의 M2M100 모델을 사용하여 영어 문장을 한국어로 번역하는 코드

```python
# 필요한 라이브러리 및 모델 불러오기
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

# 사용할 M2M100 모델 이름 지정
model_name = "facebook/m2m100_418M"

# M2M100 토크나이저와 모델을 로드
tokenizer = M2M100Tokenizer.from_pretrained(model_name)
model = M2M100ForConditionalGeneration.from_pretrained(model_name)

# 번역할 문장 정의
sentence = "The quick brown fox jumps over the lazy dog"

# 입력 문장을 토큰화하고 PyTorch 텐서로 변환
encoded_sentence = tokenizer(sentence, return_tensors="pt")

# 소스 언어를 영어("en")로 설정하고 대상 언어를 한국어("ko")로 설정
tokenizer.src_lang = "en"
model.config.forced_bos_token_id = tokenizer.get_lang_id("ko")

# 번역 수행 (영어 -> 한국어)
generated_tokens = model.generate(**encoded_sentence)

# 번역된 결과를 디코딩 (특수 토큰 제외)
translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)

# 번역 결과 출력
print(f"Translated text: {translated_text}")
```
1. **모델과 토크나이저 로드**   
&emsp;M2M100Tokenizer와 M2M100ForConditionalGeneration을 로드합니다. facebook/m2m100_418M은 다국어 간 번역을 지원하는 418M 파라미터 모델입니다.   

2. **번역할 문장 정의**   
&emsp;번역할 문장을 sentence에 저장합니다.   

3. **문장 토큰화**   
&emsp;tokenizer를 사용해 문장을 토큰화하여 PyTorch 텐서 형식으로 변환합니다. 이 변환된 데이터는 모델의 입력값으로 사용됩니다.   

4. **소스 언어와 대상 언어 설정**   
- tokenizer.src_lang에 소스 언어(여기서는 영어)를 설정합니다.   
- model.config.forced_bos_token_id에 번역 대상 언어의 ID를 설정합니다. 이 예제에서는 대상 언어를 한국어로 지정합니다.   

5. **번역 수행**   
&emsp;model.generate를 호출하여 번역된 텍스트의 토큰을 생성합니다.   

6. **번역 결과 디코딩**   
&emsp;generated_tokens를 tokenizer.decode를 통해 디코딩하여 읽을 수 있는 텍스트로 변환합니다. skip_special_tokens=True는 결과에서 특수 토큰을 제거하는 옵션입니다.   

7. **번역 결과 출력**   
&emsp;최종 번역된 문장을 출력합니다.   

---

#### NLLB-200 모델을 사용하여 영어 문장을 한국어로 번역하는 코드
```python
# 필요한 라이브러리 및 모델 임포트
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# 사용할 NLLB-200 모델 이름 지정
model_name = "facebook/nllb-200-distilled-600M"

# NLLB-200 모델과 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 번역할 문장 정의
sentence = "The quick brown fox jumps over the lazy dog"

# 입력 문장을 토크나이저로 토큰화하고 PyTorch 텐서로 변환
inputs = tokenizer(sentence, return_tensors="pt")

# forced_bos_token_id에 한국어("kor_Hang")의 토큰 ID를 설정
generated_tokens = model.generate(
    inputs.input_ids, 
    forced_bos_token_id=tokenizer.convert_tokens_to_ids("kor_Hang"), 
    max_length=30
)

# 번역 결과를 디코딩하여 읽을 수 있는 텍스트로 변환 (특수 토큰 제외)
translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)

# 번역된 텍스트 출력
print(f"Translated text: {translated_text}")
```
1. **모델과 토크나이저 로드**   
&emsp;AutoTokenizer와 AutoModelForSeq2SeqLM을 통해 NLLB-200 모델(facebook/nllb-200-distilled-600M)과 해당 토크나이저를 불러옵니다. 이 모델은 200개 이상의 언어 쌍 간 번역을 지원합니다.   

2. **번역할 문장 정의**   
&emsp;번역할 문장을 sentence에 저장합니다.   

3. **문장 토큰화**   
&emsp;tokenizer를 사용해 문장을 토큰화하여 PyTorch 텐서 형식으로 변환합니다. 이 변환된 데이터는 모델의 입력값으로 사용됩니다.   

4. **번역 수행**   
- forced_bos_token_id 매개변수를 통해 번역 대상 언어를 한국어로 설정합니다. 여기서는 "kor_Hang"을 사용하여 한국어 번역을 지정합니다.   
- max_length=30으로 설정해 출력 문장의 최대 길이를 30으로 제한합니다.   

5. **번역 결과 디코딩**   
&emsp;번역된 토큰 시퀀스를 tokenizer.decode를 통해 사람이 읽을 수 있는 한국어 텍스트로 변환합니다. skip_special_tokens=True는 결과에서 특수 토큰을 제거합니다.   

6. **번역 결과 출력**   
&emsp;최종 번역된 문장을 출력합니다.   

---

#### 사전 학습된 BERT 모델을 사용하여 IMDb 데이터셋의 일부를 테스트 및 영화 리뷰 분류에 대한 정확도를 평가하는 코드
&emsp;여기서는 BERT 모델을 추가 학습(fine-tuning) 없이 바로 사용하는 방식입니다.   
```python
# 필요한 라이브러리 임포트
from transformers import BertTokenizer, BertForSequenceClassification
from datasets import load_dataset
import torch
import numpy as np
from sklearn.metrics import accuracy_score

# IMDb 데이터셋 로드
dataset = load_dataset("imdb")

# 무작위성을 위해 시드를 설정한 후, 테스트 데이터셋에서 500개 샘플만 선택하여 축소
test_dataset = dataset['test'].shuffle(seed=42).select(range(500))

# BERT 토크나이저 로드
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# 각 샘플의 'text' 필드를 토크나이징하며, 문장을 최대 길이로 패딩하고 필요 시 잘라냄
def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# 테스트 데이터셋에 토크나이징 적용
test_dataset = test_dataset.map(tokenize_function, batched=True)

# 'input_ids', 'attention_mask', 'label' 필드만 포함하고 데이터 형식을 PyTorch 텐서로 지정
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# 이 BERT 모델은 이진 분류(긍정/부정)를 위한 SequenceClassification 목적으로 사용됨
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# 평가 모드로 설정하여 드롭아웃 비활성화
model.eval()

# 예측 및 정확도 평가를 위한 변수 초기화
all_preds = []
all_labels = []

# 테스트 데이터셋을 8개 배치로 나누어 처리
for batch in torch.utils.data.DataLoader(test_dataset, batch_size=8):
    with torch.no_grad():  # 그래디언트 비활성화 (추론 전용)
        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])
    logits = outputs.logits  # 예측된 로짓(logits) 추출
    preds = np.argmax(logits.numpy(), axis=1)  # 각 샘플의 로짓에서 가장 높은 값을 가진 인덱스 추출 (0 또는 1)
    all_preds.extend(preds)  # 예측값 저장
    all_labels.extend(batch['label'].numpy())  # 실제 레이블 저장

# 정확도 계산
accuracy = accuracy_score(all_labels, all_preds)
print(f"Accuracy without fine-tuning: {accuracy:.4f}")
```
1. **imdb 데이터셋 로드**   
&emsp;load_dataset("imdb")를 통해 Hugging Face Datasets 라이브러리에서 IMDb 영화 리뷰 데이터셋을 로드합니다.

2. **테스트 데이터셋 축소**   
&emsp;테스트 세트에서 500개의 샘플을 무작위로 선택하여 사용합니다. shuffle 함수에서 seed를 설정하여 무작위성을 제어합니다.   

3. **토크나이저 로드 및 함수 정의**   
&emsp;BERT의 BertTokenizer를 사용하여 영화 리뷰 텍스트를 토크나이징합니다. tokenize_function 함수는 각 샘플의 텍스트를 최대 길이로 패딩하고, 문장이 길면 잘라냅니다.   

4. **데이터셋 토크나이징 및 포맷 설정**   
- map을 사용해 tokenize_function을 데이터셋에 적용하고, 토큰화된 데이터를 모델 입력으로 사용할 수 있게 준비합니다.   
- 필요한 컬럼만 남기고 데이터셋을 PyTorch 텐서 형식으로 설정합니다.   

5. **모델 로드 및 평가 모드 설정**   
- BERT 모델을 이진 분류(num_labels=2)로 설정합니다. 이때, 긍정과 부정 리뷰로 분류됩니다.   
- model.eval()을 사용하여 모델을 평가 모드로 전환하고, 드롭아웃 레이어를 비활성화합니다.   

6. **예측 수행 및 평가**   
- torch.utils.data.DataLoader를 사용하여 테스트 데이터를 배치 단위로 나누어 모델에 입력합니다.   
- torch.no_grad()를 사용해 그래디언트를 비활성화하여 추론만 수행합니다.   
- 모델의 출력을 통해 예측된 로짓에서 가장 큰 값을 가진 인덱스를 추출해 각 샘플의 예측 결과로 사용합니다.   

7. **정확도 계산**   
&emsp;accuracy_score 함수를 사용해 실제 라벨과 예측된 라벨 간의 정확도를 계산하고, 결과를 출력합니다.   

&emsp;이 코드는 파인튜닝 없이 사전 학습된 BERT 모델을 사용해 기본적인 리뷰 분류 성능을 평가